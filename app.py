import streamlit as st
import requests
import tempfile
import os
import tensorflow as tf

MODEL_URL = "https://github.com/Imaaneea/cat_dog_classification_images/blob/master/APP%20deploiement/cats_and_dogs_model.tflite"

@st.cache_resource  # Utilisation de st.cache_resource
def load_model():
    """T√©l√©charge et charge le mod√®le TFLite"""
    with st.spinner("T√©l√©chargement du mod√®le..."):
        response = requests.get(MODEL_URL, stream=True)
        if response.status_code == 200:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".tflite") as temp_model_file:
                for chunk in response.iter_content(chunk_size=1024):
                    temp_model_file.write(chunk)
                model_path = temp_model_file.name
        else:
            st.error("Erreur lors du t√©l√©chargement du mod√®le.")
            return None

    # V√©rifier si le fichier existe et sa taille
    if os.path.exists(model_path):
        st.write(f"‚úÖ Mod√®le t√©l√©charg√© √† : {model_path}")
        st.write(f"üìÇ Taille du fichier : {os.path.getsize(model_path)} octets")
    else:
        st.error("‚õî Le fichier du mod√®le n'a pas √©t√© t√©l√©charg√© correctement.")
        return None

    # Charger le mod√®le TensorFlow Lite
    try:
        model = tf.lite.Interpreter(model_path=model_path)
        model.allocate_tensors()
        st.success("‚úÖ Mod√®le charg√© avec succ√®s !")
        return model
    except Exception as e:
        st.error(f"‚õî Erreur lors du chargement du mod√®le : {str(e)}")
        return None

# Charger le mod√®le
model = load_model()
